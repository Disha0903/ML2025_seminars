{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u72PQwTVRKQw"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adasegroup/ML2025_seminars/blob/main/Seminar_02/Seminar_2_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RsAmCqRKHcod"
      },
      "source": [
        "# Seminar 2: Regression"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uPoE09W6lSP2"
      },
      "source": [
        "# Content :\n",
        "\n",
        "In this seminar we cover different aspects of the one of two main problem statements in machine learning: a regression problem!\n",
        "\n",
        "In particular we consider:\n",
        "\n",
        "1. Parameter estimation for real & noisy observations\n",
        "2. How to choose right estimates: - regression model quality metrics\n",
        "3. Regularization: LASSO and Ridge regressions\n",
        "4. Kernel regression\n",
        "5. Overfitting for regression"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a3PQ6LBlfbmc"
      },
      "source": [
        "# Regression problem statement\n",
        "\n",
        "We consider a sample of pairs $D = {(\\mathbf{x}_i, y_i)}_{i = 1}^N$. Input features $\\mathbf{x}_i \\in \\mathbb{R}^d$, output $y_i \\in \\mathbb{R}$.\n",
        "\n",
        "The goal is to construct a regression model $\\hat{y}(\\mathbf{x}_i)$ that predicts the output using input features:\n",
        "$$\n",
        "\\hat{y}(\\mathbf{x}_i) \\approx y_i.\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU78zp5s-qdI"
      },
      "outputs": [],
      "source": [
        "# required imports\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "font = {'size'   : 20}\n",
        "plt.rc('font', **font)\n",
        "label_font_size = 18\n",
        "legend_font_size = 18\n",
        "\n",
        "np.random.seed(4242)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BSOEadGE-qdJ"
      },
      "source": [
        "# 1. Estimating from real & noisy observations.\n",
        "\n",
        "## A simple linear case with noise.  \n",
        "\n",
        "In this example, we have data collected from some device. We try to measure $v_0$ and $a$ in the following equation:\n",
        "\n",
        "$$ v = v(x) = v_0 + a x, \\  x \\in [20, 120]$$\n",
        "But our device is not perfect and has some measurement error, some of the values contain noise, so let's assume that the coefficients are random variables:\n",
        "$$ a \\sim  \\mathcal{N}(3, 1), \\   v_0 \\sim \\mathcal{N}(7, 5) $$\n",
        "\n",
        "Our goal is to **build an accurate predictive model** as a function $\\hat{v}(x)$ regardless the measurement error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oxF7Jpx-qdL"
      },
      "outputs": [],
      "source": [
        "# set the parameters with respect to the problem statement\n",
        "points = np.arange(20, 120)\n",
        "\n",
        "v0_real = 7\n",
        "a_real = 3\n",
        "v_real = v0_real + a_real * points # observations without noise\n",
        "\n",
        "a = np.random.normal(loc=a_real, scale=1, size=points.shape)\n",
        "v0 = np.random.normal(loc=v0_real, scale=5, size=points.shape)\n",
        "\n",
        "v = v0 + a * points # observations with noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9cR01wl8-qdP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "plt.xlabel('Input, $x$', fontsize=label_font_size)\n",
        "plt.ylabel('Output, $v(x)$', fontsize=label_font_size)\n",
        "\n",
        "plt.scatter(points, v, label='Real data', color='g')\n",
        "plt.legend(fontsize=legend_font_size);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_CnszbSsbf7Q"
      },
      "source": [
        "### Where can we find such observations?\n",
        "\n",
        "`< Your ideas>`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sXihfD84b-zP"
      },
      "source": [
        "### We have noisy measurements, wo what do we want from them?\n",
        "\n",
        "*We want to find a function which describes the underlying process!*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pTOB0QVx-qdQ"
      },
      "source": [
        "- We need to solve a regression problem: recover or approximate $v_0$ and $a$ from given observations. <br>\n",
        "- With two parameters $v_0$ and $a$ we can plot a line which will be our estimate.\n",
        "\n",
        "- What is the best line? And how to choose the best line? Since  $v_0$ and $a$ can take an infinite number of values, we have an infinite number of lines / an infinite number of functions $v(x) = v_0 + a x$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KLBt1SVLm2zr"
      },
      "source": [
        " # 2. How to choose right estimates: regression quality metrics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GhQvtHjpTNaH"
      },
      "source": [
        "**How to choose the right parameters? What line is better?**\n",
        "\n",
        "$\\hat{v}$ is our estimation here with parameters plugged manually from manual inspection of the plots.\n",
        "From now let's pretend we do not know true values of $a$ and $v_0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d2D-rd6kTNaI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "plt.xlabel('Input, $x$', fontsize=label_font_size)\n",
        "plt.ylabel('Output, $v(x)$', fontsize=label_font_size)\n",
        "\n",
        "v_estimate_one = 8 + 3.5 * points\n",
        "v_estimate_two = 6 + 3 * points\n",
        "v_estimate_three = 7 + 2 * points\n",
        "\n",
        "plt.plot(points, v_estimate_one, label='$\\hat{v}_1(x)$ = 8 + 3.5 x ', color='r', lw=2);\n",
        "plt.plot(points, v_estimate_two, label='$\\hat{v}_2(x)$ = 6 + 3 x', color='b', lw=2);\n",
        "plt.plot(points, v_estimate_three, label='$\\hat{v}_3(x)$ = 7 + 2 x', color='magenta', lw=2);\n",
        "plt.scatter(points, v, label='Real data', color='g')\n",
        "plt.legend(fontsize=legend_font_size);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9YEpmL2p-qdW"
      },
      "source": [
        "To find the best fit, we measure distances from estimated lines to the true points and choose one with the smallest average distance or one with the smallest average squared distance.\n",
        "\n",
        "$N$ - number of observations. <br>\n",
        "$y_i$ - real (true) data points. <br>\n",
        "$\\hat{f}$ - our predictions <br>\n",
        "\n",
        "Here is the most popular distances for regression task Mean Squared Error (MSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE):\n",
        "$$\n",
        "MSE(\\hat{f}, D) = \\frac{1}{N} \\sum_{i=1}^{N}\\left(\\ y_i - \\hat{f}(x_i)\\ \\right) ^2,\n",
        "$$\n",
        "\n",
        "$$\n",
        "MAE(\\hat{f}, D) = \\frac{1}{N} \\sum_{i=1}^{N}\\left|\\ y_i - \\hat{f}(x_i)\\ \\right|,\n",
        "$$\n",
        "\n",
        "$$\n",
        "MAPE (\\hat{f}, D) = \\frac{100}{N} \\sum_{i=1}^{N}\\left| \\frac{y_i - \\hat{f}(x_i)}{y_i} \\right|.\n",
        "$$\n",
        "\n",
        "\n",
        "For now let's use $MSE$. In our case real or observed values are $v(x_i)$ and $\\hat{v}(x_i)$ is our model's predictions with some parameters $a$ and $v_0$.\n",
        "\n",
        "$$\n",
        "MSE(v, \\hat{v}) = \\frac{1}N \\sum_{i = 1}^{N} (v(x_i) - \\hat{v}(x_i))^2.\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_0QK-JWJenri"
      },
      "source": [
        "### What measures or a metric we should choose and when? Please propose other metrics.\n",
        "\n",
        "`< Your ideas>`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5E6pY5-7ofIn"
      },
      "source": [
        "### Let's illustrate how $MSE$ looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YiNURr0R-qdW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "plt.scatter(points[20:30], v[20:30], s=200, c='g', alpha=1, label='$v(x)_{observed}$')\n",
        "plt.plot(points[20:30], v_real[20:30], label='$\\hat{v}(x)$')\n",
        "\n",
        "#plt.scatter(t[20:30], v[20:30], s=200, c='b', label='y')\n",
        "\n",
        "for _point, _v in zip(points[20:30], v[20:30]):\n",
        "    plt.plot([_point, _point], [_v, a_real * _point + v0_real], c='r')\n",
        "\n",
        "plt.title(\"MSE - Illustrated\")\n",
        "plt.legend(fontsize=legend_font_size);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f8i99VsFTNaO"
      },
      "source": [
        "Let's try to estimate our parameters $v_0$ and $a$ naively by \"eye\" and see what error we get (let's pretend we do not know their real values).\n",
        "\n",
        "But to estimate the parameters we need to implement this function first $MSE(v,\\hat{v})= \\frac{1}N \\sum_i^{N} (v(t_i) - \\hat{v}(t_i))^2 $."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWNSq2dtlG-M"
      },
      "outputs": [],
      "source": [
        "# Your task: implement MSE function\n",
        "\n",
        "def mse(v_true, v_pred):\n",
        "    # v_true - a vector of true values\n",
        "    # v_pred - a vector of predictions\n",
        "\n",
        "    return  #MSE (single value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4bxB3jYX-qdY"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "plt.xlabel('Input, $x$', fontsize=label_font_size)\n",
        "plt.ylabel('Output, $v(x)$', fontsize=label_font_size)\n",
        "\n",
        "v_estimate_one = 8 + 3.5 * points\n",
        "v_estimate_two = 6 + 3 * points\n",
        "v_estimate_three = 7 + 2 * points\n",
        "plt.plot(points, v_estimate_one, label=\"$\\hat{v}$\" + f\"= 8 + 3.5 x, MSE: {mse(v, v_estimate_one)}\", color='r', lw=0.5);\n",
        "plt.plot(points, v_estimate_two, label=\"$\\hat{v}$\" + f\"= 6 + 3 x, MSE: {mse(v, v_estimate_two)}\", color='b', lw=0.5);\n",
        "plt.plot(points, v_estimate_three, label=\"$\\hat{v}$\" + f\"= 7 + 2 x, MSE: {mse(v, v_estimate_three)}\", color='y', lw=0.5);\n",
        "#plt.plot(points, y_real, label='real', color='r', lw=0.5);\n",
        "plt.scatter(points, v, label='Data with error', color='g')\n",
        "plt.legend();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CmlP31dzTNaR"
      },
      "source": [
        "In this toy example it was easy to find the right coefficients.  For multdimensional input it is impossible, as the search space is large. <br>\n",
        "Let's try to find more accurate coefficients and compute them. We need to choose such parameters $v_0$ and $a$ such that our MSE value is as small as possible for a test sample"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "clHviYlNfDsu"
      },
      "source": [
        "### Ok, let's focus on MSE and find optimal parameters for it"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DNnGu3Ek-qda"
      },
      "source": [
        "We will be more formal from now on and define:\n",
        "$$\\hat{v}(x) = v_0 + a x =  \\hat{f}(x) = a x + b$$\n",
        "\n",
        "\n",
        "1. We have $X$ and $\\mathbf{y}$: $\\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots, (\\mathbf{x}_n, y_n)\\}$, $X = (\\mathbf{x}_1, \\ldots, \\mathbf{x}_n)^T$, $ \\mathbf{y}= (y_1, \\ldots, y_n)^T$.\n",
        "2. And we want to find $\\hat{f}(\\mathbf{x})$ to minimize\n",
        "$$MSE(\\hat{f}, D) = \\frac{1}{N} \\sum_{i=1}^{N}\\left(\\ y_i - \\hat{f}(\\mathbf{x}_i)\\ \\right) ^ 2 \\rightarrow \\text{min}$$\n",
        "3. We assume that $\\hat{f}(x)$ is linear:\n",
        "$$\\hat{f}(x) = ax + b.$$\n",
        "\n",
        "----\n",
        "Now we plug  $\\hat{f}(x)$ in MSE and get:\n",
        "$$\n",
        "\\frac{1}{N} \\sum_{i=1}^{N}\\left(\\ y_i - ax_i - b\\right) ^ 2 \\rightarrow \\text{min}_{a, b}.\n",
        "$$\n",
        "\n",
        "There are two options to find $a$ and $b$:\n",
        "1. **Analytically in vector** form: take the first derivative, find parameters that lead to zero value of the first derivative.\n",
        "2. Find partial derivatives with respect to $a$ and $b$ and apply a **gradient descent.**\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7El8Wcsrfswg"
      },
      "source": [
        "### Analytical solution"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lu3VCmYW-qdb"
      },
      "source": [
        "We will set $X$ as an array with observed values in the second column and ones in the first, so $X$ has shape $N$ x $2$, where $N$ is the number of observations.\n",
        "Then we will write our parameters $a$ and $b$ as a vector $\\theta = (b, a)^T$, then we have:\n",
        "\n",
        "$$\n",
        "\\hat{f}(x) = ax + b.\n",
        "$$\n",
        "\n",
        "For the whole sample we have:\n",
        "\n",
        "$$\n",
        "\\hat{f}(X) = X \\theta.\n",
        "$$\n",
        "\n",
        "Again, $X \\in \\mathbb{R}^{N \\times 2}; \\theta \\in \\mathbb{R}^{2}$. Now, let us move to the derivations.\n",
        "\n",
        "1. After we plug this vectorised form into MSE we get:\n",
        "\n",
        "$$\n",
        "\\frac{1}{N}(\\mathbf{y} - X \\theta)^{T} (\\mathbf{y} - X \\theta) \\rightarrow \\min_{\\theta}\n",
        "$$\n",
        "\n",
        "2. Take the derivative with respect to  $\\theta$ and make it equal to 0 (*Hint*: take the advantage of [matrix_cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)):\n",
        "$$\n",
        "X^T\\mathbf{y} = X^T X \\theta\n",
        "$$\n",
        "\n",
        "3. Matrix $X^T X$ is almost always invertible, therefore we can compute $\\theta$:\n",
        "$$\n",
        "\\hat{\\theta} = (X^T X)^{-1} X^T \\mathbf{y}\n",
        "$$\n",
        "\n",
        "**Again**, first we compute $\\theta = (X^T X)^{-1} X^T \\mathbf{y}$, then we can use it to estimate $\\hat{f}(x_{\\text{new}}) = x_{\\text{new}}^T \\theta $. Note, that in vector form $\\theta = (b,a)^T$ and one dimention (column) of $X$ should have all 1, that will serve as a multiplier coefficient in front of $b$.\n",
        "\n",
        "Now your task is to implement a function which computes $\\theta$ .\n",
        "\n",
        "(This equation for $\\theta$ is called **Normal equation**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8xrFEdduQMI"
      },
      "outputs": [],
      "source": [
        "# YOUR TASK IS TO COMPUTE THETA\n",
        "\n",
        "def compute_theta(X, y):\n",
        "    # X is a vector of values\n",
        "    # y is a vector of values\n",
        "\n",
        "    # EXPAND OUR X WITH ONES FOR BIAS TERM\n",
        "    x_with_ones = np.ones(shape = (X.shape[0], 2))\n",
        "    x_with_ones[:, 1] = X\n",
        "\n",
        "    theta = ...\n",
        "\n",
        "    # COMPUTE THETA TIPS:\n",
        "    # np.linalg.inv(matrix) - use to compute inverse of a matrix\n",
        "    # np.dot(matrix_one, matrix_two) - use for matrix multiplication\n",
        "\n",
        "    # theta =\n",
        "    return theta"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hOa5IwoX-qdb"
      },
      "source": [
        "Now we compute our estimates for $\\theta$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "69tNbXFG-qdc"
      },
      "outputs": [],
      "source": [
        "print(compute_theta(points, v))\n",
        "b, a = list(compute_theta(points, v))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2k2EKkDfTNaT"
      },
      "source": [
        "We can see that we have good estimate for $a$ but not that good estimate for $v_0$. We will discuss this phenomenon later.\n",
        "\n",
        "Let's plot them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VoFm7PjmTNaU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.xlabel('Input, $x$', fontsize=label_font_size)\n",
        "plt.ylabel('Output, $v(x)$', fontsize=label_font_size)\n",
        "\n",
        "v_estimate = b + a * points\n",
        "plt.plot(points, v_estimate, label=f\"Estimated {round(b, 3)} + {round(a, 3)} * x | MSE: {mse(v, v_estimate)}\", color='k', lw=1.5);\n",
        "plt.scatter(points, v, label='Data with error', color='g')\n",
        "plt.legend(fontsize=legend_font_size);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mnMB5Ounwp2t"
      },
      "source": [
        "Remember that $a$ and $v_0$ have the following distributions:\n",
        "\n",
        "$$ a \\sim  \\mathcal{N}(3, 1), \\   v_0 \\sim \\mathcal{N}(7, 5) $$\n",
        "\n",
        "We have got not bad results for $a$. <br>\n",
        "\n",
        "Why results for $v_0$ are much worse? Your ideas?\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aBaYv5QDTNaU"
      },
      "source": [
        "Also, we can see that is still better than our naive approach \"plugging values on the eye\". However, in real life we have only a small subset of data. Let's see if we can do well only with a half of our observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mcBFlHWYTNaU"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_points_number = 10\n",
        "\n",
        "training_points = np.random.choice(points.shape[0], training_points_number,  replace=False)\n",
        "\n",
        "print(compute_theta(points[training_points], v[training_points]))\n",
        "b, a = compute_theta(points[training_points], v[training_points])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GyHhpJ3lTNaW"
      },
      "source": [
        "### Estimates only on half of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uyhkArXwTNaW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.xlabel('Input, $x$', fontsize=label_font_size)\n",
        "plt.ylabel('Output, $v(x)$', fontsize=label_font_size)\n",
        "\n",
        "v_estimate = b + a * points\n",
        "plt.plot(points, v_estimate, label=f\"Estimated v = {round(b, 3)}+{round(a, 3)} x | MSE: {mse(v, v_estimate)}\", color='k', lw=1.5);\n",
        "plt.plot(points, v_estimate_one, label=f\"Manual v = 8 + 3.5 x | MSE: {mse(v, v_estimate_one)}\", color='r', lw=0.5);\n",
        "#plt.plot(points, y_real, label='real', color='r', lw=0.5);\n",
        "plt.scatter(points, v, label='Real data', color='g')\n",
        "\n",
        "plt.legend(fontsize=legend_font_size);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D2cRtlJgTNaW"
      },
      "source": [
        "It's clear that given only the half of the observation we get worse MSE.\n",
        "\n",
        "*Please make conclusions!*\n",
        "\n",
        "But can we do something about?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XpQSiVzhx8Vj"
      },
      "source": [
        "# 3. Regularization: LASSO and Ridge regressions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j2s2hN7C-qdc"
      },
      "source": [
        "# Ridge regression\n",
        "\n",
        "We try to improve our estimate by removing the effect of \"very noisy\" samples.\n",
        "One of the ways to do so is to penalize big values for our estimates of $a$ and $b$ or the vector $\\theta$ in general."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qUlc4byt-qdc",
        "outputId": "47ede1f6-d3db-491a-ef7d-769fe78964c9"
      },
      "source": [
        "$$\n",
        "MSE(\\mathbf{y}, X\\mathbf{\\theta}) + \\lambda\\|\\mathbf{\\theta}\\|_2^2=  \\|\\mathbf{y} - X\\mathbf{\\theta}\\|^2_2 + \\lambda\\|\\mathbf{\\theta}\\|^2_2\n",
        "$$\n",
        "\n",
        "It's possible to solve it using gradient descend,but we will use an analytical solution for this case again:\n",
        "\n",
        "$$\n",
        "\\mathbf{\\theta}^*_{\\text{reg}} = (X^T X + \\lambda I_p)^{-1}X^T \\mathbf{y},\n",
        "$$\n",
        "where $I_p$ is a diagonal matrix consisting of ones ($p = 2$ : dimensionality of $I_p$), and $\\lambda$ is a regularisation coefficient.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "seu7FG0lTNaX"
      },
      "source": [
        "# Lasso regression\n",
        "\n",
        "$$\n",
        "MSE(\\mathbf{y}, X \\mathbf{\\theta}) + \\lambda\\|\\mathbf{\\theta}\\|_1=  \\|\\mathbf{y} - X \\mathbf{\\theta}\\|^2_2 + \\lambda\\|\\mathbf{\\theta}\\|_1.\n",
        "$$\n",
        "\n",
        "The main difference here is that LASSO regression has $l_1$ norm instead of $l_2$ norm for the regularisation term. LASSO solution has no analytical form, but can be found via a gradient descent.\n",
        "\n",
        "Below we will work through some examples with Ridge regression. Note, that the difference may seem small, but leads to quite different results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MAt8h-SymFF"
      },
      "outputs": [],
      "source": [
        "# AGAIN YOUR TASK HERE IS TO COMPUTE THETA AS IN EQUATION ABOVE FOR THETA\n",
        "\n",
        "def compute_theta_regularized(X, y, lambda_value):\n",
        "    # X is a vector of values\n",
        "    # y is a vector of values\n",
        "    # lambda_value is a regularization coefficient\n",
        "\n",
        "    # EXPAND OUR X WITH ONES FOR BIAS TERM\n",
        "    x_with_ones = np.ones(shape = (X.shape[0], 2))\n",
        "    x_with_ones[:, 1] = X\n",
        "\n",
        "    n_features = x_with_ones.shape[1]\n",
        "    lambda_on_diagonal_matrix = lambda_value * np.eye(n_features)\n",
        "\n",
        "    theta = ...\n",
        "\n",
        "    # COMPUTE THETA TIPS:\n",
        "    # np.linalg.inv(matrix) - use to compute inverse of a matrix\n",
        "    # np.dot(matrix_one, matrix_two) - use for matrix multiplocation\n",
        "\n",
        "    # theta =\n",
        "    return theta\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ocg8DXFaTNaX"
      },
      "source": [
        "Here $\\lambda$ is our hyperparameter which we usually need to optimize, and value of $\\lambda = 0$ should give us the same solution as one we had before - let's do it as a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iwfy5yHO-qdd"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_points_number = 100\n",
        "\n",
        "training_points = np.random.choice(points.shape[0], training_points_number,  replace=False)\n",
        "\n",
        "print(compute_theta_regularized(points[training_points], v[training_points], 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0FzJYlCYTNaZ"
      },
      "outputs": [],
      "source": [
        "font = {'size'   : 10}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "f, ax = plt.subplots(12, 1, figsize=(15, 50))\n",
        "\n",
        "lambdas = [0, 1, 2, 3, 10, 20, 30, 50, 100, 300, 500, 1000]\n",
        "\n",
        "\n",
        "for i in range(len(lambdas)):\n",
        "    b, a  = list(compute_theta_regularized(points[training_points],\n",
        "                                           v[training_points], lambdas[i]))\n",
        "    v_estimate = b + a * points\n",
        "    ax[i].plot(points, v_estimate, label=f\"Estimate v = {round(b, 3)} + {round(a, 3)} x | MSE: {mse(v, v_estimate)}\", color='k', lw=1.5);\n",
        "    #plt.plot(t, y_real, label='real', color='r', lw=0.5);\n",
        "    ax[i].scatter(points, v, label='Observed data', color='g')\n",
        "    ax[i].set_title(f'lambda  = {lambdas[i]}');\n",
        "    ax[i].legend();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bVljjzjfTNaa"
      },
      "source": [
        "**Lasso Regression**\n",
        "\n",
        "To show the difference between Lasso and Ridge we will do the same estimations.\n",
        "We use LASSO regression from sklearn. For model training we use even fewer ponits: only 15. So, our results will be different from the previous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X5lz8iaoTNaa"
      },
      "outputs": [],
      "source": [
        "font = {'size'   : 10}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "f, ax = plt.subplots(10, 1, figsize=(15, 40))\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lambdas = [0, 1, 2, 3, 10, 20, 30, 50, 100, 1000]\n",
        "\n",
        "training_points_number = 15\n",
        "\n",
        "training_points = np.random.choice(points.shape[0], training_points_number,  replace=False)\n",
        "\n",
        "for i in range(len(lambdas)):\n",
        "    b, a  = list(compute_theta_regularized(points[training_points],\n",
        "                                           v[training_points], lambdas[i]))\n",
        "    lasso = Lasso(alpha=lambdas[i],  max_iter=100, tol=0.001)\n",
        "\n",
        "    lasso.fit(points[training_points].reshape(-1, 1), v[training_points])\n",
        "    v_lasso = lasso.predict(points.reshape(-1, 1))\n",
        "    v_estimate = b + a * points\n",
        "    ax[i].plot(points, v_estimate, label=f\"Ridge a={round(a, 3)}  b={round(b, 3)} MSE: {mse(v, v_estimate)}\", color='g', lw=1.5);\n",
        "    ax[i].plot(points, v_lasso, label=f\"Lasso a={round(float(lasso.coef_), 3)}  b={round(float(lasso.intercept_), 3)} MSE: {mse(v_lasso, v)}\",\n",
        "               color='k', lw=1.5);\n",
        "    ax[i].scatter(points, v, label='Observed', color='g')\n",
        "    ax[i].set_title(f'lambda  = {lambdas[i]}');\n",
        "    ax[i].legend();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y3HfzBscTNaa"
      },
      "source": [
        "We see that both give different results based on $\\lambda$ values, and it is difficult to say which one is better. However, we can notice that with high lambda values, Lasso tends to zero our coefficient while Ridge reduces the slope value - our coefficient but does not zero it."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UKQS6xzlTNab"
      },
      "source": [
        "We just used regularization term and improved our estimates for $a$, the average value we have got is around 3.3, which is quite close for our initial value. Even though our estimate for $v_0$ is not good enough due to a lot of initial noise, it is still a good result.\n",
        "\n",
        "\n",
        "Well done!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7tAT_VY-qde"
      },
      "source": [
        "# 3. Kernels\n",
        "\n",
        "What if our initial function was more complex than $ v = v(x) = v_0 + a x$? Could we still use the same tools and get good results? Let's try for the following equation:\n",
        "$$ D(x) = \\cos(\\sqrt{g / l} \\cdot x) + \\epsilon$$\n",
        "\n",
        "This time we will have noisy $$ \\epsilon =  \\mathcal{N}(0, 0.5)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6WZfOJDJ-qde"
      },
      "outputs": [],
      "source": [
        "np.random.seed(4242)\n",
        "\n",
        "font = {'size'   : 20}\n",
        "plt.rc('font', **font)\n",
        "\n",
        "def D(g, l, t):\n",
        "    return np.cos(np.sqrt(g / l) * t)\n",
        "\n",
        "points = np.arange(0, 100)\n",
        "\n",
        "l_real = 100\n",
        "g_real = 1\n",
        "\n",
        "d_real = D(g_real, l_real, points)\n",
        "\n",
        "eps = np.random.normal(0, scale=0.5, size=points.shape)\n",
        "\n",
        "d = d_real + eps\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "plt.scatter(points, d, label='Observed', color='b')\n",
        "plt.plot(points, d_real, label='Real', color='r')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('D(x)')\n",
        "plt.legend();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iGBejLUUG6Uq"
      },
      "source": [
        "Consider applying linear model for the data above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UD9BdaTpTNab"
      },
      "outputs": [],
      "source": [
        "print(compute_theta_regularized(points, d, 0))\n",
        "b, a = list(compute_theta_regularized(points, d, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Q4rTtYCeTNac"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.ylabel('D(x)')\n",
        "plt.xlabel('x')\n",
        "d_estimate = b - a * points\n",
        "plt.plot(points[:100], d_estimate[:100], label=f\" Analytical  | MSE: {mse(d, d_estimate)}\",\n",
        "         color='k', lw=1.5);\n",
        "plt.scatter(points, d, label='observed', color='g')\n",
        "plt.legend();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IWSsczCwTNac"
      },
      "source": [
        "Well, it's clearly not what we want here and we need somehing more complicated than linear assumption"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t-rGE4zETNad"
      },
      "source": [
        "The easiest way to solve it is to add polynomial features to our observations which create non-linear dependencies, let's try it. Now we need to create polynomial features and find coefficients for each one. To compute our estimates or model predictions, we will use the following equation:\n",
        "\n",
        "\n",
        "$$\\hat{f}(x) = a_1 x^1 + a_2 x^2 + \\ldots a_p x^p + b = \\theta^{T} X $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYsKhozqTNad"
      },
      "outputs": [],
      "source": [
        "# this functions returns weights together with new polynomial features\n",
        "\n",
        "def make_polynomial_features(x_train, p):\n",
        "    poly = np.zeros(shape = (len(x_train), p + 1))\n",
        "    poly[:, 0] = 1\n",
        "    for i in range(1, p + 1):\n",
        "        poly[:, i] = np.power(x_train, i).reshape((len(x_train),))\n",
        "    return poly\n",
        "\n",
        "def ridge_polynomial_regression(x_poly, y, lambda_value):\n",
        "    theta = np.linalg.pinv((x_poly.T.dot(x_poly) + lambda_value * np.eye(x_poly.shape[1]))).dot(x_poly.T).dot(y)\n",
        "    return theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7qGEJZLTNad"
      },
      "outputs": [],
      "source": [
        "x_poly = make_polynomial_features(points, p=3)\n",
        "theta = ridge_polynomial_regression(x_poly, d, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "k_qJOKlqTNae"
      },
      "outputs": [],
      "source": [
        "print(theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "l8IlvkrpTNaf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "y_estimate_p = x_poly.dot(theta)\n",
        "plt.plot(points, y_estimate_p, label=f\" Analytical  | MSE: {mse(d, y_estimate_p)}\", color='k', lw=1.5);\n",
        "#plt.plot(points, y_real, label='real', color='r', lw=0.5);\n",
        "plt.scatter(points, d, label='Noisy', color='g')\n",
        "plt.plot(points, d_real, label='Real', color='r')\n",
        "plt.legend();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rDXcoowQTNah"
      },
      "source": [
        "We created polynomial features on the base on our data. Our assumption was that the highest degree of 3 provides the best results. But what if our assumption is wrong, and our data samples have no polynomial relationship?\n",
        "\n",
        "Well, we can use other **\"Kernels\"** to account for it."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mUsMcmfjRKRG"
      },
      "source": [
        "### Let us recall some general stuff about usage of kernels:\n",
        "\n",
        "**Feature mapping**: (note that $x_i \\in \\mathbb{R}$)\n",
        "\n",
        "$\\phi : \\mathbb{R} \\rightarrow \\mathbb{H}$;\n",
        "\n",
        "**Kernel function**:\n",
        "\n",
        "$k: \\mathbb{R}\\times \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
        "\n",
        "It defines *scalar product* in Hilbert space $\\mathbb{H}$: $\\langle \\phi(x), \\phi(x')\\rangle_{\\mathbb{H}} = k(x, x')$.\n",
        "\n",
        "**Ridge regression with kernel**:\n",
        "\n",
        "1. Dual form of ridge regression\n",
        "  $$\\hat{f}(\\mathbf{x}_{\\text{new}}) = \\mathbf{x}_{\\text{new}}^T \\mathbf{\\theta}^*_{\\text{reg}} = \\mathbf{x}_{\\text{new}}^T (X^T X + \\lambda I_p)^{-1}X^T \\mathbf{y} = \\mathbf{x}_{\\text{new}}^T X^T (X X^T + \\lambda I_n)^{-1} \\mathbf{y},$$\n",
        "  where $n$ is the number of training points (recall that $X = (\\mathbf{x}_1, \\dots, \\mathbf{x}_n)^T$).\n",
        "\n",
        "2. Consider $\\mathbf{x}_i \\rightarrow \\phi(\\mathbf{x}_i)$ ; $\\langle \\cdot, \\cdot \\rangle \\rightarrow \\langle \\cdot, \\cdot \\rangle_{\\mathbb{H}}$. Then,\n",
        "$$\\hat{f}(\\mathbf{x}_{\\text{new}}) = k(x_{\\text{new}})^T (K + \\lambda I_n)^{-1} \\mathbf{y},$$\n",
        "where\n",
        "$$K = \\begin{bmatrix}\\langle \\phi(\\mathbf{x}_{1}), \\phi(\\mathbf{x}_1) \\rangle_{\\mathbb{H}} & \\dots & \\langle \\phi(\\mathbf{x}_{1}), \\phi(\\mathbf{x}_n) \\rangle_{\\mathbb{H}} \\\\ \\vdots & \\vdots & \\vdots \\\\ \\langle \\phi(\\mathbf{x}_{n}), \\phi(\\mathbf{x}_1) \\rangle_{\\mathbb{H}} & \\dots & \\langle \\phi(\\mathbf{x}_{n}), \\phi(\\mathbf{x}_n) \\rangle_{\\mathbb{H}} \\end{bmatrix} = \\begin{bmatrix} k(\\mathbf{x}_{1}, \\mathbf{x}_1) & \\dots & k(\\mathbf{x}_{1}, \\mathbf{x}_n) \\\\ \\vdots & \\vdots & \\vdots \\\\ k(\\mathbf{x}_{n}, \\mathbf{x}_1) & \\dots & k(\\mathbf{x}_{n}, \\mathbf{x}_n) \\end{bmatrix}$$\n",
        "is *kernel* (or kernel matrix) - SPSD;\n",
        "$$k(\\mathbf{x}_{\\text{new}}) = \\begin{pmatrix}\\langle \\phi(\\mathbf{x}_{\\text{new}}), \\phi(\\mathbf{x}_1) \\rangle_{\\mathbb{H}} \\\\ \\vdots \\\\  \\langle \\phi(\\mathbf{x}_{\\text{new}}), \\phi(\\mathbf{x}_n) \\rangle_{\\mathbb{H}}\\end{pmatrix} = \\begin{pmatrix} k(\\mathbf{x}_{\\text{new}}, \\mathbf{x}_1) \\\\ \\vdots \\\\  k(\\mathbf{x}_{\\text{new}}, \\mathbf{x}_n)\\end{pmatrix} $$\n",
        "is *kernel* evaluated at new point $\\mathbf{x}_{\\text{new}}$.\n",
        "\n",
        "*Question:* Can we combine kernels $k_1, k_2, \\dots$? What will change in the formulas above?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6YIfODxnRKRG"
      },
      "source": [
        "### Examples of kernels\n",
        "\n",
        "One of the common kernels is RBF (Radial Basis Function) kernel, let's see how it works in practice.\n",
        "https://en.wikipedia.org/wiki/Radial_basis_function_kernel"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_3UWFKEkTNai"
      },
      "source": [
        "<!-- We skip the deriviation of the kernel method for now except a few important equations:\n",
        "$$\n",
        "\\hat{f} = \\theta^T\\phi(\\mathbf{x})\n",
        "$$\n",
        "\n",
        "$\\phi(x)$ is our non linear mapping function.\n",
        "\n",
        "$$\n",
        "\\theta^T \\phi(\\mathbf{x}) = \\mathbf{y} (\\phi \\phi^T + \\lambda I)^{-1} \\phi^T \\phi(\\mathbf{x}),\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta^T\\phi(x) = \\mathbf{y} (K + \\lambda I)^{-1} k_k,\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{f} = \\mathbf{y} (K + \\lambda I)^{-1} k_k,\n",
        "$$\n",
        "\n",
        "$K$ is our kernel function. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FM8Q-8m_TNaj"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "def rbf_kernel(X, gamma=1):\n",
        "    '''Evaluates the RBF kernel for all pairs of rows in X'''\n",
        "    return np.exp(-gamma * squareform(pdist(X.T, 'sqeuclidean')));\n",
        "\n",
        "def kernel_ridge_solution(X, y, l=1, gamma=1):\n",
        "    \"\"\" Given a kernel, compute y_hat\"\"\"\n",
        "    x_with_ones = np.ones(shape = (X.shape[0], 2))\n",
        "    x_with_ones[:, 1] = X\n",
        "    K = rbf_kernel(x_with_ones.T, gamma)\n",
        "    W = K.dot(np.linalg.inv(K + l * np.eye(x_with_ones.shape[0])))\n",
        "    yhat = W.dot(y)\n",
        "    return yhat, K, W\n",
        "\n",
        "yhat, K, H = kernel_ridge_solution(points, d, l=2, gamma=0.01)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(points, yhat, label=f\" Analytical  | MSE: {mse(d, yhat)}\", color='k', lw=1.5);\n",
        "#plt.plot(points, y_real, label='real', color='r', lw=0.5);\n",
        "plt.scatter(points, d, label='Noisy', color='g')\n",
        "plt.plot(points, d_real, label='Real', color='r')\n",
        "plt.legend(fontsize=legend_font_size);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R1GqaY0RTNaj"
      },
      "source": [
        "It's different from true observations, but quite good, and now we can model a rather general class of nonlinear dependencies."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w2pQu1tZ3JJO"
      },
      "source": [
        "### An example of overfitting\n",
        "\n",
        "Kernels and Addition of polynomial features help to approximate our true nonlinear function.\n",
        "\n",
        "Would it be a good idea to add more polynomial features, and when should we stop?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezNKebzbQScz"
      },
      "outputs": [],
      "source": [
        "x_poly = make_polynomial_features(points, p=120)\n",
        "theta = ridge_polynomial_regression(x_poly, d, 0)\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "y_estimate_p = x_poly.dot(theta)\n",
        "plt.plot(points, y_estimate_p, label=f\" Analytical  | MSE: {mse(d, y_estimate_p)}\", color='k', lw=1.5);\n",
        "#plt.plot(points, y_real, label='real', color='r', lw=0.5);\n",
        "plt.plot(points, d_real, label='Real', color='r');\n",
        "plt.scatter(points, d, label='Noisy', color='g')\n",
        "plt.legend(fontsize=legend_font_size);"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yxtXAHQpQpNn"
      },
      "source": [
        "## Well, we see that order of 120 we have a very low error.\n",
        "## But did we recover our inital dependency?\n",
        "## How to choose the best order of polynomial for generated features?\n",
        "<hr>\n",
        "\n",
        "### To answer these questions, we need to consider real world scenarios.\n",
        "### 1. Usually, in real life problems we deal with multidimensional problems - a lot of features. It means we can't plot them and see, if we have a good fit or not.\n",
        "### 2. Also, often we have some training data and we need to know, how well we will predict at unseen data.\n",
        "\n",
        "\n",
        "### One way to solve this problem is\n",
        "1. Split our data in three parts\n",
        "- train\n",
        "- validation\n",
        "- test\n",
        "\n",
        "2. Find optimal $\\theta$ using the training part\n",
        "3. Find optimal order of polynimial features with validation by minimizing MSE on the validation part\n",
        "4. Check MSE values on the test part - we assume that the error on test shows, how well we predict/esimate on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RoqyS2wQ0Jj"
      },
      "outputs": [],
      "source": [
        "# spit our data in three parts\n",
        "train_points, train_d = points[0::3], d[0::3]\n",
        "val_points, val_d = points[1::3], d[1::3]\n",
        "test_points, test_d = points[2::3], d[2::3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBfCB1qmbLv3"
      },
      "outputs": [],
      "source": [
        "test_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSDH4EcHba6D"
      },
      "outputs": [],
      "source": [
        "val_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1wwhHtJbWvt"
      },
      "outputs": [],
      "source": [
        "train_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL2m1I_6UvWY"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(15,10))\n",
        "\n",
        "\n",
        "x_train = make_polynomial_features(train_points, p=100)\n",
        "theta = ridge_polynomial_regression(x_train, train_d, 0)\n",
        "\n",
        "y_train = x_train.dot(theta)\n",
        "ax[0].plot(train_points, y_train, label=f\" Training  | MSE: {mse(y_train, train_d)}\", color='k', lw=1.5);\n",
        "ax[0].plot(points[0::3], d_real[0::3], label='Real', color='r');\n",
        "ax[0].scatter(points[0::3], d[0::3], label='Noisy', color='g')\n",
        "ax[0].set_title(f\" Training  | MSE: {mse(y_train, train_d)}\")\n",
        "\n",
        "x_val = make_polynomial_features(val_points, p=100)\n",
        "y_val = x_val.dot(theta)\n",
        "ax[1].plot(val_points, y_val, label=f\" Validation  | MSE: {mse(y_val, val_d)}\", color='k', lw=1.5);\n",
        "ax[1].plot(points[1::3], d_real[1::3], label='Real', color='r');\n",
        "ax[1].scatter(points[1::3], d[1::3], label='Noisy', color='g')\n",
        "ax[1].set_title(f\" Validation  | MSE: {mse(y_val, val_d)}\")\n",
        "\n",
        "plt.legend();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wcjF-xv-W_Uw"
      },
      "source": [
        "### The effect we observe here is overfitting.\n",
        "\n",
        "### Your assignemnt now is to find such order of polynomial feauters that our error on validation test is minimal.\n",
        "### Note, you can use train data to estimate values for $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8xEggTgeKqW"
      },
      "outputs": [],
      "source": [
        "# # ASSIGNMENT\n",
        "# # FIND THE P - order of poylinomials which minimiezes error on validation part of dataset\n",
        "\n",
        "best_p = ...\n",
        "best_mse = ...\n",
        "\n",
        "print(f' Best  p {best_p}, best MSE on validation {best_mse}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKDdMcG3WmOC"
      },
      "outputs": [],
      "source": [
        "P = 6\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(20,10))\n",
        "\n",
        "\n",
        "x_train = make_polynomial_features(train_points, P)\n",
        "theta = ridge_polynomial_regression(x_train, train_d, 0)\n",
        "\n",
        "y_train = x_train.dot(theta)\n",
        "ax[0].plot(train_points, y_train, label=f\" Train  | MSE: {mse(y_train, train_d)}\", color='k', lw=1.5);\n",
        "ax[0].plot(points[0::3], d_real[0::3], label='Real', color='r');\n",
        "ax[0].scatter(points[0::3], d[0::3], label='Noisy', color='g')\n",
        "ax[0].set_title(f\" Train  | MSE: {mse(y_train, train_d)}\")\n",
        "\n",
        "x_val = make_polynomial_features(val_points, P)\n",
        "y_val = x_val.dot(theta)\n",
        "ax[1].plot(val_points, y_val, label=f\" Validation  | MSE: {mse(y_val, val_d)}\", color='k', lw=1.5);\n",
        "ax[1].plot(points[1::3], d_real[1::3], label='Real', color='r');\n",
        "ax[1].scatter(points[1::3], d[1::3], label='Noisy', color='g')\n",
        "ax[1].set_title(f\" Validation  | MSE: {mse(y_val, val_d)}\")\n",
        "\n",
        "x_test = make_polynomial_features(test_points, P)\n",
        "y_test = x_test.dot(theta)\n",
        "ax[2].plot(test_points, y_test, label=f\" Validation  | MSE: {mse(y_test, test_d)}\", color='k', lw=1.5);\n",
        "ax[2].plot(points[2::3], d_real[2::3], label='Real', color='r');\n",
        "ax[2].scatter(points[2::3], d[2::3], label='Noisy', color='g')\n",
        "ax[2].set_title(f\" Test  | MSE: {mse(y_test, test_d)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qQQz_ke8e8co"
      },
      "source": [
        "1. How can you explain this effect?\n",
        "2. Why do we need to split our data in three parts?\n",
        "3. What our MSE error will be likelly on unseen data from the same distribution?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT-wxJ6_d4xR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
